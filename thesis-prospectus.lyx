#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing onehalf
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\rightmargin 3cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Semantic Composition and Models of Linguistic Comprehension
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
This thesis is concerned with the problem of ascribing meanings to expressions
 in a natural language.
 Traditionally, philosophers have attempted to solve this problem by proposing
 views on which a meaning is a particular kind of entity (e.g.
 a proposition, a referent, a mental state etc.).
 The job of a semantic theory, as such, is to pair linguistic expressions
 with the entities that constitute their meanings.
 I argue for an alternative view on which meaning attributions are simply
 concise ways of expressing predictive generalizations about cognitive and
 behavioral phenomena.
 To give an account of the meaning of a linguistic expression, I claim,
 is simply to give a set of expectations concerning the cognitive and behavioral
 effects of its use.
 This empirically-oriented approach to semantics gives rise to a number
 of theoretical criteria that I outline in Chapter 1.
 In Chapter 2, I use these criteria to evaluate a number of contemporary
 semantic theories.
 I argue that the most common problem facing these theories is that they
 are unable to explain how the meanings of simple and complex linguistic
 expressions are related to one another in a predictively adequate way.
 To help solve this problem, Chapter 3 reinterprets the principle of composition
ality as a claim about the generalization of linguistic abilities rather
 than as a claim about the properties of linguistic representations.
 
\end_layout

\begin_layout Standard
With the stage set for a new approach, I use Chapter 4 to develop a formal
 framework for describing the meanings of linguistic expressions.
 This framework describes such phenomena both in terms of abstract input-output
 functions computed by a cognitive system, and in terms of processes within
 the system that serve to implement these computations.
 Specifically, I argue that attributing linguistic comprehension to a cognitive
 system is roughly tantamount to hypothesizing that the system satisfies
 a particular functional description.
 By decomposing this functional description in manner consistent with a
 recent account of cognitive architecture, I illustrate how to analyze linguisti
c phenomena across multiple levels of abstraction.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
A caveat is that I largely avoid any discussion of the important and highly
 complex social aspects of linguistic phenomena in this thesis.
\end_layout

\end_inset

 In Chapter 5, I put this analysis to the test by developing a cognitive
 model of question answering that is designed exhibit a rudimentary form
 of linguistic comprehension.
 The key feature of the model is that it 
\end_layout

\begin_layout Standard
The remainder of the thesis is concerned with performing empirical and theoretic
al evaluations of my work.
 In Chapter 6, I test the model on some widely used question-answering datasets
 and compare its performance to a number alternatives developed in the machine
 learning community.
 In Chapter 7, I discuss a number of outstanding philosophical and formal
 issues.
 Specifically, I illustrate how my framework is both consistent with standard
 descriptions of inferentialist semantic theories and resistant to the criticism
s typically lodged against these theories.
 I also illustrate how the framework is consistent with different mathematical
 interpretations (e.g.
 probabilistic, control-theoretic, etc.).
 I conclude, finally, with a brief discussion of directions for future research.
 
\end_layout

\begin_layout Section*
1.
 A Cognitive Approach to Semantics
\end_layout

\begin_layout Quote
In order to say what a meaning 
\shape italic
is
\shape default
, we may first ask what a meaning 
\shape italic
does
\shape default
, and then find something that does that.
 - David Lewis, 1970
\end_layout

\begin_layout Standard
There seems to be no shortage of things for meanings to do.
 On the one hand, they are given the rather theoretical job of connecting
 words to objects in the world and specifying the truth conditions of sentences.
 On the other hand, they are given the rather practical job of explaining
 linguistic and cognitive behavior.
 Philosophers have put a lot of effort into finding entities that are able
 to satisfy these job descriptions, but to relatively little avail.
 In practice, more or less distinct theories have been developed to achieve
 more or less distinct goals.
 For example, theories in formal semantics are designed to account for truth
 and reference, but not for linguistic behavior.
 Theories in pragmatics and psychology, by comparison, are designed to do
 roughly the opposite.
 As such, there appears to be two largely independent notions of meaning
 at play in the philosophical literature: the first notion - call it 
\begin_inset Quotes eld
\end_inset

metaphysical semantics
\begin_inset Quotes erd
\end_inset

 - is concerned with relations that obtain between representations and reality,
 while the second notion - call it 
\begin_inset Quotes eld
\end_inset

cognitive semantics
\begin_inset Quotes erd
\end_inset

 - is concerned with explaining how people comprehend and produce linguistic
 expressions.
\end_layout

\begin_layout Standard
My goal in this chapter is to motivate a cognitive approach to semantics
 and outline a corresponding set of theoretical desiderata.
 There are two main reasons for favoring the cognitive approach.
 First, it accords well with a naturalistic conception of philosophy.
 To a naturalist, meanings can be thought of as entities posited by a scientific
 theory that aims to explain how and why we use linguistic expressions in
 the way that we do.
 Attributing a meaning to word is not unlike attributing a center of mass
 to an object or charge to a particle (cf.
 Dennett, 1987); in all such cases, the attribution is warranted because
 lends explanatory and predictive insight into certain phenomena of interest
 (i.e.
 phenomena involving words, objects, and particles respectively).
 Second, very little is lost by giving up on the considerations that have
 motivated the metaphysical approach.
 Placing reference and truth at the core of semantic theory has yielded
 well-known theoretical considerations on which the meaning of an expression
 or mental state can depend on facts entirely unknown to language users
 (e.g.
 Twin earth cases, Frege cases, etc).
 I argue that such considerations give rise to unsatisfiable and counterproducti
ve theoretical demands.
 Put simply, allowing distinctions in meaning to correspond to obscure distincti
ons in reality burdens semantic theory with a dependency on facts about
 the world that have no empirical consequences concerning language use.
 Moreover, communal ignorance of such facts problematically translates into
 communal ignorance about the meanings of ordinary words and sentences.
\end_layout

\begin_layout Standard
As a result of these considerations favoring the cognitive approach, I propose
 a set of four criteria that can be used to evaluate competing semantic
 theories:
\end_layout

\begin_layout Enumerate
The predictive adequacy criterion.
\end_layout

\begin_layout Enumerate
The implementation criterion.
 
\end_layout

\begin_layout Enumerate
The compositionality criterion.
\end_layout

\begin_layout Enumerate
The scalabilty criterion.
\end_layout

\begin_layout Standard
While these criteria are somewhat distinct from those commonly found in
 the literature, they should not be entirely surprising.
 The first criterion is meant to capture the idea that the attribution of
 a particular meaning to a linguistic expression is warranted only when
 doing so has scientific value.
 Such value, I contend, arises when the meaning attributed to a particular
 expression gives rise to successful predictions concerning (a) the cognitive
 states of individuals who produce or comprehend the expresssion, and (b)
 the verbal behavior of such individuals.
 To illustrate with an example, consider the following sentence: “The boy
 waited for the pitch and then hit the ball over the fence”.
 A competent speaker of English is able to rapidly infer from this sentence
 that the boy is likely playing baseball, that he has used a bat to hit
 the ball, and that the phrase “over the fence” refers to where the ball
 went after he hit it, rather than to the location of the ball when he hit
 it.
 The predictive adequacy criterion favors attributing a meaning to this
 sentence that entails that anyone who correctly understands it will provide
 appropriate answers to questions such as 
\begin_inset Quotes eld
\end_inset

Where did the ball go?
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

What did the boy likely use to hit the ball?
\begin_inset Quotes erd
\end_inset

 Finally, it is worth emphasizing that these predictions are most naturally
 understood in probabilistic rather than deductive terms: a meaning attribution
 does not 
\shape italic
entail
\shape default
 the truth of certain conterfactual conditionals concerning linguistic or
 cognitive phenomena; rather it 
\shape italic
favors
\shape default
 such conditionals in a violable manner.
\end_layout

\begin_layout Standard
The second criterion is meant to capture the idea that predictions licensed
 by a particular meaning attribution place certain requirements on things
 to which the predictions are applied (e.g.
 cognitive systems).
 For example, if a meaning attribution specifies that a certain phrase is
 likely to evoke a certain behavior in a listener, then the word and the
 behavior must be related such that this specification is met.
 However, there might be compelling evidence to suggest that a cognitive
 system is simply unable to mediate between words and behaviors in the specified
 way.
 Such evidence ought to prompt a revision to the proposed meaning attribution.
 Similarly, if there is more compelling evidence to suggest that the behavioral
 expectations in question actually obtain, then a revision to one's account
 of cognitive processing ought to be considered.
 Overall, the implementation criterion simply favors the development of
 theories of linguistic behavior and linguistic cognition that are consistent
 with one another.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
As a brief technical aside, it can be useful to think of the implementation
 criterion through analogy to the Neural Engineering Framework (NEF).
 Models built using the NEF typically assume that neural populations are
 responsible for computing transformations on an implicit vector space through
 their neural activities.
 Hypothesizing that a particular neural ensemble computes a particular transform
ation is highly analogous to making a meaning attribution.
 To explain, the hypothesis generates a large number of predictions concerning
 the values that can be decoded from the ensemble under various conditions.
 Likewise, the hypothesis also places constraints on the properties of the
 neurons in the ensemble.
 For instance, their tuning curves need to tile the vector space in a manner
 that permits computing the hypothesized function.
 (These tuning curves are determined both by the intrinsic properties of
 the neurons and their incoming connection weights).
 Overall, just as evidence concerning function and implementation mutually
 constrain one another in the development of neurocomputational theories,
 so too do they constraint one another in the development of semantic theories.
 This all should come as no surprise given the representational considerations
 that underlie the NEF.
 To explain, treating the vector that characterizes a neural population's
 state as a 
\begin_inset Quotes eld
\end_inset

representation
\begin_inset Quotes erd
\end_inset

 only makes sense in the presence of futher assumptions about the system
 in which that population resides.
 Namely, assumptions need to be made about the other groups of neurons to
 which this population is connected, and about relations between the behaviors
 of these neurons and certain external events (e.g.
 the presence or absence of a particular kind of stimulus).
 Content attributions, put simply, go hand-in-hand with expectations about
 how a system functions, regardless of whether the system is a simple population
 of neurons or an entire brain.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The third criterion is widely accepted in discussions of semantics, and
 is meant to account for the fact that people are able to generalize their
 understanding of the meanings of simple linguistic expressions (and syntax)
 to an understanding of arbitrarily large numbers of more complicated expression
s.
 In typical discussions of semantics, this productive capacity is characterized
 in terms of the recursive application of a finite (and therefore learnable)
 set of semantic and syntactic rules.
 While I do not wish to adhere strictly to this traditional picture, I do
 wish to maintain that any adequate semantic theory has to provide some
 explanation of how the meanings of complex expressions (e.g.
 sentences) are related to the meanings of their simpler parts (e.g.
 words and phrases).
 
\end_layout

\begin_layout Standard
The scalability criterion, finally, gives preference to semantic theories
 that account for the meanings of large numbers of linguistic expressions.
 It is of course not particularly controversial to say that scientific theories
 with a broad range of empirical coverage are preferable to scientific theories
 with a smaller range of coverage.
 But when considered in tandem with the other criteria listed above, scalability
 concerns become pressing.
 For example, some approaches to semantics are able to achieve compositionality
 along with a certain degree of predictive adequacy, but only in highly
 restricted domains.
 A salient case would be work in formal semantics that handles a limited
 range phenomena focused on things like generalized quantification and coordinat
ion.
 At best, this approach covers a small fraction of the possible linguistic
 expression types, and efforts to increase the scope of the approach have
 been largely unsuccessful.
\end_layout

\begin_layout Standard
In the remainder of the chapter, I consider two objections the selection
 of these criteria.
 The first objection maintains that they favor a kind of instrumentalism
 that is typical of theories that prioritize predictive adequacy over other
 concerns (see e.g.
 Dennett, 1987).
 My response is that if the distinctions between two competing meaning attributi
ons have no empirical basis, then it is unclear that there reasons to prefer
 one over the other.
 In other words, I am taking Lewis to heart and claiming that if two competing
 meaning attributions 
\shape italic
do
\shape default
 the same work, then for all intents and purposes, they 
\shape italic
are
\shape default
 the same.
 Overall, as a point of methodological principle, it is simply unwise to
 burden a theory with empirically superfluous material.
 
\end_layout

\begin_layout Standard
The second objection claims that the cognitive approach entails a form of
 meaning relativism.
 Again, I am willing to concede the point to an extent.
 Words do mean different things to different people, because usage of a
 given word can sustain different predictive generalizations in different
 individuals and different social groups.
 However, the fact that language use is a social practice militates against
 widespread relativism.
 People communicate to achieve practical goals, and achieving these such
 goals requires that people be able to make good predictions in relation
 to the usage of particular linguistic expressions; hence, people converge
 on shared patterns of usage that foster predictive success.
 As such, a tempered form of relativism is to be expected rather than avoided.
 
\end_layout

\begin_layout Standard
In conclusion, the main purpose of this chapter is to set the stage for
 the development of an approach to semantics that is both philosophically
 robust and integrated with the broader research goals of cognitive science.
 
\end_layout

\begin_layout Section*
2.
 Formal Methods and the State of the Art
\end_layout

\begin_layout Quote
We aren’t happy about semantics.
 - Jerry Fodor and Ernest Lepore, 2012
\end_layout

\begin_layout Standard
In what follows, I survey a number of existing approaches to describing
 the meanings of natural language expressions.
 I highlight the strengths and weaknesses of these approaches and provide
 a brief summary of open problems that need to be solved in order to achieve
 progress.
\end_layout

\begin_layout Standard
The analysis proceeds in three stages.
 First, I evaluate contemporary work in formal semantics.
 This work stems largely from Montague's (1970) pioneering insight that
 model-theoretic intepretations of the sort applied to formal languages
 can also be applied to natural languages (cf.
 Partee, 1980).
 To explain, a model-theoretic interpretation defines a mapping between
 a set of linguistic expressions and a set of mathematical objects.
 In the case of a first order language, the objects in question are drawn
 from sets of individuals and truth values, along with functions and relations
 defined on these sets (Goldfarb, 2003; Carpenter, 1997).
 Once each simple linguistic term is interpreted in this way, a truth value
 is assigned every sentence in the language.
 According to most formal semanticists, the meaning of sentence is captured
 by the class of interpretations that make it true (or, equivalently, the
 class of models that satisfy the sentence).
 It is for this reason that 
\begin_inset Quotes eld
\end_inset

truth-conditional semantics
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

formal semantics
\begin_inset Quotes erd
\end_inset

 are largely synonymous terms.
\end_layout

\begin_layout Standard
To their credit, formal semanticists have produced influential analyses
 of a variety of linguistic phenomena involving things like quantification,
 tense, modality, and anaphora.
 But it should be obvious that this work is unsatisfactory from the perspective
 of the criteria introduced in Chapter 1.
 Most importantly, denotational theories typically have little predictive
 adequacy.
 They are simply not designed to account for the cognitive and behavioral
 aspects of language use (Jackendoff, 2004; Partee, 1980).
 To illustrate with an example, a standard denotation to give the word 
\begin_inset Quotes eld
\end_inset

table
\begin_inset Quotes erd
\end_inset

 is the set of things that are tables.
 Needless to say, this set of objects does not give rise to the sort of
 expectations that would satisfy the predictive adequacy criterion unless
 numerous further assumptions are made concerning how these objects are
 represented in the minds of language users.
 Some see this abstraction away from empirical considerations as a virtue
 (e.g.
 Lewis, 1970), while others see it as a flaw in need of a remedy (e.g.
 Partee, 1980; Jackendoff, 2004).
 
\end_layout

\begin_layout Standard
Nonetheless, it is important to note that the methods of formal semantics
 are not intrinsically incapable of producing good explanations of cognitive
 and linguistic phenomena.
 These methods are committed to the idea that meanings are denotations,
 but the denotations in question need not be mathematical objects: they
 could be states in a cognitive model, computer programs, linguistic expressions
, or any number of other things (Liang & Potts, 2015).
 Recent work has exploited this looser notion of denotation in tandem with
 state-of-the-art machine learning techniques to develop systems that parse
 questions expressed in natural language onto 
\begin_inset Quotes eld
\end_inset

logical forms
\begin_inset Quotes erd
\end_inset

 that denote appropriate answers extracted from a knowledge base (Liang,
 Jordan, & Klein, 2013).
 Most impressively, the mapping from linguistic expressions to logical forms
 is learned strictly from examples of correct pairings between expressions
 and their denotations.
 Such learning procedures might be naturally extended to develop cognitive
 models that correctly mediate between certain inputs (linguistic expressions)
 and their denotations (other linguistic expressions) in an empirically
 adequate way.
 Overall, I conclude from this evaluation that formal semantics has certain
 strengths that can inform the development of any adequate theory.
 It does very well with respect to compositionality, and recent efforts
 have shown promise with respect to scalability.
 The weaknesses, on the other hand, are still somewhat prohibitive.
 Predictive adequacy and implementational plausibility exist only in the
 realm of speculation.
 
\end_layout

\begin_layout Standard
In the next stage of my analysis, I examine what are referred to as 
\begin_inset Quotes eld
\end_inset

distributional
\begin_inset Quotes erd
\end_inset

 models of meaning.
 Motivated by the idea that a word's meaning can be captured by 
\begin_inset Quotes eld
\end_inset

the company it keeps
\begin_inset Quotes erd
\end_inset

 (Firth qtd.
 in Grefenstette, 2013), these models are typically generated by encoding
 word co-occurence information from text corpora into a high-dimensional
 vector space (Clark, 2014; Turney & Pantel, 2010; Landauer & Dumais, 1997).
 The vectors that result from this process possess geometric relationships
 that capture semantic relationships of various kinds.
 For instance, the vectors for words associated with a particular abstract
 category (e.g.
 sports, food, animals, etc.) will tend to cluster together.
\end_layout

\begin_layout Standard
Distributional models of meaning have proven quite effective at accounting
 for a wide range of empirical data concerning word usage.
 They have achieved success matching human data from studies of category
 typicality (e.g., Jones & Mewhort, 2007) and synonym identification (e.g.,
 Landauer & Dumais, 1997).
 Good matches to human data on tasks involving stem-completion (Jones &
 Mewhort, 2007) and judgments of phrasal similarity (Mitchell & Lapata,
 2010) have also been achieved.
 Given these matches, it is clear that the distributional approach does
 reasonably well at satisfying the predictive adequacy criterion.
 And given that distibutional models involving tens of thousands of words
 generated from millions of documents have been developed, it is also clear
 that the approach can scale effectively.
 However, a major shortcoming of these models is that sentences and paragraphs
 are often treated as a 
\begin_inset Quotes eld
\end_inset

bag of words
\begin_inset Quotes erd
\end_inset

 with no synactic structure.
 More generally, it has proven very difficult to generate compositional
 representations of phrases and sentences using the distributional approach.
 
\end_layout

\begin_layout Standard
However, it also true that a great deal of effort has been directed towards
 incorporating compostionality into these models.
 For instance, there is a long history of using 
\begin_inset Quotes eld
\end_inset

binding
\begin_inset Quotes erd
\end_inset

 methods to combine distributed representations of distinct words or concepts.
 Smolenksy’s (1990) tensor products, Plate’s (2003) holographic reduced
 representations, and Sahlgren et al's (2010) randomly permuted vectors
 are perhaps the most well-known techniques of this kind.
 However, there is a growing consensus that these methods do not compose
 representations in a predictively adequate way.
 The key problem is that simply joining two representations together does
 not always yield an appropriate compound representation.
 For example, binding a representation for the word 
\begin_inset Quotes eld
\end_inset

tall
\begin_inset Quotes erd
\end_inset

 to a representation for the word 
\begin_inset Quotes eld
\end_inset

tree
\begin_inset Quotes erd
\end_inset

 does not create a compound representation that accurately characterizes
 tall trees.
 Rather, the compound representation is only interpretable in terms of the
 fact that it can be used to recover each of its constituent parts.
 Binding, overall, is a syntactic operation that lacks an appropriate semantic
 counterpart.
\end_layout

\begin_layout Standard
In light of this problem, more recent efforts to achieve compositionality
 have made use of either (a) structured neural networks that function to
 merge distributed representations (e.g.
 Weston et al., 2014; 2015, Socher et al., 2012), or (b) tensor-based representati
ons that can composed via operations analogous to the sort used by formal
 semanticists (e.g.
 Grefenstette, 2013; Clark, 2014).
 I briefly evaluate these approaches and argue that, while they are effective
 at solving specific kinds of problems, they do not provide general purpose
 representations of composite linguistic structures.
 The properties of the representations in question are optimized to achieve
 a very specific goal that is unrelated to most of the phenomena that a
 meaning attribution for a complex expression ought to account for.
 Put simply, compositionality is obtained in these models at the price of
 predictive adequacy and scope.
 
\end_layout

\begin_layout Standard
Finally, I evaluate work from the 
\begin_inset Quotes eld
\end_inset

connectionist
\begin_inset Quotes erd
\end_inset

 literature that more focused on uncovering the principles underlying linguistic
 cognition than on practical linguistic applications.
 I focus primarily on two bodies of work.
 First, I discuss Rogers and McClelland's (2004, 2008) influential work
 on 
\begin_inset Quotes eld
\end_inset

semantic cognition
\begin_inset Quotes erd
\end_inset

.
 Second, I discuss Smolensky's (2006, 2014) work on harmonic grammar and
 the integration of symbolic and connectionist descriptions of cognitive
 architecture.
 
\end_layout

\begin_layout Standard
In the case of Rogers and McClelland's work, a relatively simple multi-layered
 neural network is trained via backpropogation to associate abstract representat
ions of perceptual inputs with abstract representations of the properties
 that are characteristic of these inputs.
 For example, a perceptual representation of BIRD would evoke representations
 of numerous predictates that apply to birds, such as CAN_FLY and HAS_WINGS.
 Importantly, the mapping performed by the network is mediated by learned,
 distributed representations of each percept.
 This use of distributed representations allows the model to generalize
 the mappings it has learned to novel inputs.
 For instance, a perceptual representation of a more specific type of bird
 (e.g.
 FALCON) would be associated with the same predicates as BIRD (i.e.
 CAN_FLY and HAS_WINGS) due to similarity between the distributed representation
s for these two concepts.
 As Rogers & McClelland illustrate, this model has a considerable amount
 of predictive power, since it is able to account for a wide range of data
 concerning categorization, taxonomy induction, and lingustic inference.
 The model is also loosely consistent with implementational constraints
 provided by cognitive systems given that it makes use of neuron-like processing
 units.
 Given as much, the model does a reasonably good job of satsifying criteria
 (1) and (2).
 But virtues aside, the model clearly fails to address the compositionality
 criterion: it provides no description of a mechanism for composing representati
onal states.
 I briefly survey similar connectionist accounts of linguistic phenomena
 (Elman, 1990, Plaut et al., 1996, Seidenberg, 1997) and draw the same conclusion
s.
 
\end_layout

\begin_layout Standard
Smolensky's work (1990, 2006) makes use of the previously mentioned binding
 methods to incorporate structured representations into networks similar
 to those used by Rogers and McClelland.
 A key novelty of this approach is that 
\end_layout

\begin_layout Standard
Overall, I argue that this survey illustrates the existence of a persistent
 tradeoff in semantic theorizing: it is possible to satisfy either the predictiv
e adequacy criterion (e.g.
 using distributional models) or the compositionality criterion (e.g.
 using formal semantics), but it is very difficult to satisfy both at the
 same time.
 Put simply, compositional models are generally not predictively adequate,
 and predictively adequate models are generally not compositional.
 Difficulties achieving scope and implementational plausibility also tend
 to co-occur with success at achieving compositionality, as a quick comparison
 of distributional and formal approaches to semantics illustrates.
 In conclude from this that properly accounting for compositionality is
 the key challenge facing research on natural language semantics.
 
\end_layout

\begin_layout Section*
3.
 Reconsidering Compositionality
\end_layout

\begin_layout Quote
You can’t cram the meaning of a whole %&!$# sentence into a single $&!#*
 vector! - Ray Mooney, 2014
\end_layout

\begin_layout Standard
The principle of compositionality states that the meaning of a complex expressio
n is determined by the meanings of its constituents and the way they are
 combined (Szabo, 2012; Fodor & Pylyshyn, 1988).
 In the study of natural language semantics, this principle is often taken
 to entail that expressions such as phrases and sentences are interpreted
 in the following two-step manner (Recanati, 2012): first, a set of a lexical
 rules are used to assign meanings to individual words; then, a set of compositi
on rules are used to combine these simple meanings into the meaning of a
 phrase or sentence.
 
\end_layout

\begin_layout Standard
Perhaps unsurprisingly, the principle is widely seen to pose a problem for
 functional role semantics.
 Philosophers have provided numerous examples of complex linguistic expressions
 that appear to have functional roles unrelated to the functional roles
 of their constituent parts (e.g Fodor and Lepore, 1991; Fodor, 1998).
 Moreover, there is a long history of successful efforts to formalize denotation
al semantics that has simply not been matched by propents of functional
 role semantics (c.f.
 Pagin, 1997).
 This lack of theoretical progress has led to considerable skepticism about
 the viability of functionalist theories.
 
\end_layout

\begin_layout Standard
My goal in this chapter is to provide a counterweight to such skepticism.
 Specifically, I propose a view on which simple linguistic representations
 individually impose numerous constraints on a search through a space of
 entities with particular functional dispositions.
 Taken collectively, these constraints pick out an entity with a functional
 disposition that accords with the semantics of a complex linguistic expression.
 The process of optimizing over these soft constraints is hypothesized to
 occur at different levels of abstraction, allowing for compositional constructi
ons of varying degrees of complexity.
 To give an example, a determiner and a noun might together impose constraints
 on the selection of an entity with the functional disposition appropriate
 to a noun phrase.
 In turn, the resulting noun-phrase can in turn be characterized as imposing
 constraints on the selection of an entity with the functional disposition
 of a sentence.
 This sentence-level selection process can then be resolved by the introduction
 of further constraints corresponding to another sentential constituent
 (e.g.
 a verb phrase).
 Developing a semantic theory, as such, involves specifying the candidate
 entities at each level of syntactic abstraction and then defining the constrain
ts that select amongst these various candidates in various circumstances.
 A mathematically precise version of such a theory is provided in Part 2
 below.
 
\end_layout

\begin_layout Standard
In the remainder of the chapter, I give three reasons for favoring this
 constraint-based description of semantic composition.
 First, it is theoretically consistent with the criteria outlined in Chapter
 1.
 If meaning attributions are simply predictive generizations associated
 with the use of particular words and sentences, then complex meanings are
 not 
\begin_inset Quotes eld
\end_inset

built up
\begin_inset Quotes erd
\end_inset

 from simpler ones through a process analogous to the construction of wall
 from a collection of bricks (citation).
 The probabilistic expectations corresponding to a simple linguistic expression
 instead provide defeasible evidence in favor of associating certain probabilist
ic expectations with a more complex linguistic expression.
 By comparison, standard approaches to compositionality relate the meanings
 of simple and complex expressions through simple rules, but it is implausible
 to suppose that such rules can suffice for a semantic theory that satisfies
 the predictive adequacy criterion.
 To explain with an example, the semantic composition of certain adjectives
 and nouns is standardly modeled as the intersection of the sets denoted
 by each noun and adjective.
 It is highly unlikely that a comparably simple operation can be used to
 
\begin_inset Quotes eld
\end_inset

compose
\begin_inset Quotes erd
\end_inset

 the functional dispositions associated with words in these types.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Suppose, for instance, that these functional dispositions are characterized
 as sets of predictions.
 Why think that the predictions associated with a complex expression would
 be all and only the predictions shared by each of its simpler constituents?
 Suppose alternatively that the functional dispositions in question can
 be characterized as probability distributions over verbal behaviors or
 cognitive states.
 Why think that a simple rule would suffice to combine such distributions
 in the appropriate way? 
\end_layout

\end_inset

 An optimization-based approach, however, has the necessary flexibility
 to define the appropriate formal relations between these functional disposition
s.
 
\end_layout

\begin_layout Standard
Second, the constraint-based approach provides a plausible solution to the
 problem of accounting for context-sensitivity within a compositional semantics.
 An important challenge to the idea that natural languages are compositional
 arises from the fact that many expression types seem to take on different
 meanings in different contexts 
\begin_inset CommandInset citation
LatexCommand citep
key "Recanati:2005,Unnsteinsson:2014"

\end_inset

.
 Consider, for example, the following sentence: 
\begin_inset Quotes eld
\end_inset

Mike finished John's book
\begin_inset Quotes erd
\end_inset

 (Recanati, 2004, p.
 62).
 It is highly plausible that the meaning of this sentence can vary depending
 on whether Mike finished reading the book or finished binding it 
\begin_inset CommandInset citation
LatexCommand citep
key "Recanati:2005"

\end_inset

.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
The genitive construction 
\begin_inset Quotes eld
\end_inset

John's book
\begin_inset Quotes erd
\end_inset

 can also exhibit variation depending on whether, for instance, John wrote
 the book or John owns the book.
\end_layout

\end_inset

 But if the verb 
\begin_inset Quotes eld
\end_inset

finished
\begin_inset Quotes erd
\end_inset

 is assigned a fixed meaning in accordance with a semantic rule, then it
 is not clear how to account for this variation, since it cannot be attributed
 to a change in syntactic structure or a modification of the lexical constituent
s in the sentence.
 However, if it is assumed that individual words act to impose numerous
 constraints on the process of linguistic interpretation, then it follows
 that the behavior of a given word is conditioned by the other words in
 the sentence.
 As such, a word like 
\begin_inset Quotes eld
\end_inset

finished
\begin_inset Quotes erd
\end_inset

 can exhibit different kinds of compositional behavior when it occurs in
 different linguistic constructions.
 Moreover, extra-linguistic features of context can also be hypothesized
 to impose constraints on the selection process.
 For example, constraints imposed by the topic of prior discourse could
 resolve the ambiguity present in the above utterance involving John's book
 by biasing a person who heard the utterance towards responding to particular
 questions with particular answers (e.g.
 
\begin_inset Quotes eld
\end_inset

Was the book an enjoyable read?
\begin_inset Quotes erd
\end_inset

).
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
One might argue that the impact of contextual features on semantic composition
 could be accounted for by a system of rules that are sensitive to their
 context of application.
 This is true in principle, but the rules in question would have to be explicity
 defined for each possible pairing of a context and a linguistic expression.
 In practice, enumerating such a large number of rules is intractable.
 The constraint-based approach gets around this problem by having the effect
 of each hypothesized rule fall of out an optimization process that is defined
 only in relation to a fixed inventory of constraints associated with contexts
 and linguistic expressions individually (i.e.
 not considered as pairs - the difference is between 2n sets of constraints
 and 2^n rules, i.e.
 linear vs.
 exponential in the number of contexts and expressions).
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A third reason to favor a notion of compositionality that is consistent
 with the predictive adequacy criterion is that doing so usefully allows
 for 
\begin_inset Quotes eld
\end_inset

top down
\begin_inset Quotes erd
\end_inset

 effects on interpretation.
 In accordance with Frege's 
\begin_inset Quotes eld
\end_inset

context principle
\begin_inset Quotes erd
\end_inset

, one could envision a situation in which the functional disposition associated
 with a particular word during the initial interpretation of sentence differs
 from the functional disposition associated with the same word upon the
 completion of interpretation.
 Traditional accounts of semantic composition are inconsistent with this
 possibility.
 
\end_layout

\begin_layout Standard
Finally, I conclude the chapter by noting that my proposal is actually somewhat
 conservative: it does not require any radical reinterpretion the principle
 of compositionality.
 To explain, the principle merely states that there is some systematic relations
hip between the meanings of simple and complex linguistic expressions.
 To borrow an example from Szabo, meanings could be tables and chairs at
 no risk of violating the principle so long as the tables and chairs assigned
 to simple linguistic expressions are appropriately related to the tables
 and chairs assigned to more complex expressions.
 However, there seems to be a widespread assumption to contrary - namely
 that a semantic theory can only be compositional if it specifies of set
 of rules or functions that combine simple meanings into more complex ones.
 I hope to have cast doubt on this assumption by showing how it leads to
 a view of linguistic cognition that is both empirically problematic and
 theoretical restrictive.
 
\end_layout

\begin_layout Section*
4.
 A General Framework for Modeling Linguistic Phenomena 
\end_layout

\begin_layout Standard
In this chapter, I take up the task of appropriately specifying the functions
 being performed by a cognitive system that is in a particular representational
 state.
 My goal is to iteratively develop a framework that accounts for each of
 the criteria introduced in Chapter 1 while also accommodating the insights
 obtained in Chapters 2 and 3.
 
\end_layout

\begin_layout Standard
I begin by framing the predictions associated with a particular linguistic
 content attribution in terms of a set of expected question-answer pairs.
 As argued in Chapter 1, it is helpful to operationalize one's understanding
 of the meaning of the sentence in terms of the answers one would provide
 to questions about it.
 Question answering tasks are increasingly being used to benchmark semantic
 theories (see, e.g.
 Liang et al., 2013, Bordes et al., 2014, 2015; Socher et al, 2015), and can
 be applied to a wide range of expression types.
 For example, it is possible to assess whether one understands the meaning
 of the word 
\begin_inset Quotes eld
\end_inset

violin
\begin_inset Quotes erd
\end_inset

 by checking to see if one is capable of correctly answering various questions
 about violins.
 Similarly, it is possible to assess whether one understands a multi-sentence
 paragraph by checking to see if one is capable of of correctly answering
 certain questions about the paragraph (e.g.
 concerning who did what to whom).
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
As a point of clarification, I do not wish to argue that question answering
 tasks provide an exhaustive test of linguistic understanding.
 There might be good reasons to think that dispositions towards certain
 non-linguistic behaviors are also required in order for one to exhibit
 appropriate understanding.
 For instance, it would be odd to say that one understands the meaning of
 the word 
\begin_inset Quotes eld
\end_inset

violin
\begin_inset Quotes erd
\end_inset

 if one were unable to correctly identify violins or engage in certain violin-re
lated behaviors (e.g.
 retrieving violins, handling violins, etc.).
 
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
In more formal terms, the expectations associated with a meaning attribution
 can be characterized in terms of a parameterizable function that maps input
 linguistic expressions (e.g.
 questions) to output linguistic expressions (e.g.
 answers).
 The parameters that determine the behavior of the function can include
 prior linguistic input (i.e.
 the sentence that is the target of the questions) and background knowledge
 that is related to this prior input.
 From a cognitive perspective, the values of these parameters can be roughly
 identified with contents stored in short-term and long-term memory.
 An abstract model that conforms to this description is shown in Figure
 1.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /Users/peterblouw/Desktop/functionspec.png
	lyxscale 40
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
A simplified model of linguistic comprehension.
 When a word, sentence or story is provided as prior information to the
 model, it is encoded as a structure in short-term memory.
 This structure then retrieves a set of related structures from long-term
 memory.
 When a question is provided as input to the model, the structures previously
 activated in memory configure the model to compute the appropriate function,
 as indicated by the grey box around the depiction of function 
\begin_inset Quotes eld
\end_inset

F
\begin_inset script subscript

\begin_layout Plain Layout
2
\end_layout

\end_inset


\begin_inset Quotes erd
\end_inset

.
 The result of this computation is an answer that is provided as the output
 of the model.
 For example, the description of a simple baseball playing scenario encoded
 in memory would constrain the model to compute a function that maps the
 question “What did the boy use to hit the ball?” onto the answer “A baseball
 bat”.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In many respects, this abstract model is not particularly informative.
 It treats the target of the content attribution (i.e.
 a cognitive system) as a black box.
 Nonetheless, it is a good exercise to use an initial specification of this
 sort to help constrain further descriptions of the system's internals.
 However, if one's only goal is to achieve predictive adequacy with respect
 to verbal behavior, it might be sufficient to leave things at this abstract
 level of analysis.
 Many probabilistic models of cognition do just this by defining statistical
 processes that bring about some observed behavior (e.g.
 Tenenbaum et al.
 2011).
 However, these models have been criticized for their lack of attention
 to implementation details (e.g.
 Eliasmith, 2013), and for their commitment to a dubious competence-performance
 distinction in the analysis of cognitive phenomena (e.g.
 McClelland et al.
 2010).
 Overall, function specifications defined at what is sometimes referred
 to as the 
\begin_inset Quotes eld
\end_inset

computational level
\begin_inset Quotes erd
\end_inset

 of analysis are revisable starting points, not complete models.
 
\end_layout

\begin_layout Standard
Given these observations, I describe a recently developed cognitive architecture
 (Eliasmith, 2013) that, with a few adjustments, is capable of implementing
 the functions associated with sophisticated forms of linguistic cognition.
 The architecture in question, the SPA, makes use of a class of representations
 called semantic pointers that encode compressed approximations of symbol
 structures while exhibiting appropriate relationships towards a wide variety
 of perceptual and motor states.
 
\end_layout

\begin_layout Standard
The main reason for choosing the SPA is that it satisfies a number of implementa
tional constraints found in cognitive systems.
 For example, Christiansen and Chater (in press) emphasize the existence
 of a pervasive 
\begin_inset Quotes eld
\end_inset

now-or-never
\begin_inset Quotes erd
\end_inset

 bottleneck during both the comprehension and production of linguistic expressio
ns.
 On the comprehension side, the bottleneck arises due to limits on the capacity
 of working memory.
 These limits require that linguistic inputs be continually compressed and
 recoded in abstract form to make room for further items in the input stream.
 Absent such instantaneous compression, there would be no way for a comprehender
 to keep up with the ongoing barrage of words that is typical of most linguistic
 exchanges.
 Similar remarks apply in the case of the linguistic production.
 Speakers simply do not have the memory capacity to plan out ahead of time
 all of the actions associated with producing a typical sentence.
 Rather, small sub-plans are executed iteratively to bring about the production
 of the sentence.
 Notably, compression and control are hallmarks of SPA, so it stands to
 reason that the SPA can accommodate these memory and action based constraints
 associated with the 
\begin_inset Quotes eld
\end_inset

now-or-never
\begin_inset Quotes erd
\end_inset

 bottleneck.
 
\end_layout

\begin_layout Standard
Another way in which the SPA satisfies the implementation criterion is by
 decomposing abstract behavioral functions into processes involving cognitively
 and neurally plausible mechanisms.
 These mechanisms involve components that encode syntactic structure, store
 information over time, control behavior, and exhibit adaptation, amongst
 other things.
 To illustrate, it is useful to think of the Spaun model (Eliasmith et al,
 2012) as a system that satisfies certain a functional specification involving
 simple numerical representations
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
It would not be inaccurate to say that the model understands the numbers
 1 through 9 in the sense outline in Chapter 1.
 Spaun answers questions about these numbers accurately, and is generally
 able to behave as a human would with respect to a certain set of number-related
 tasks.
\end_layout

\end_inset

 using a neurallly plausible implementation.
 Spaun, in short, is a powerful demonstration of how the SPA can act as
 a bridge between descriptions of behavioral functions and the realization
 of these functions in a human brain.
 More importantly, the SPA is general enough to act as such a bridge for
 behaviors involving natural language expressions rather than simple numbers.
 Figure 2 describes Spaun-like model that could exhibit such linguistic
 behaviors.
 
\end_layout

\begin_layout Standard
The key outstanding questions 
\end_layout

\begin_layout Section*
5.
 The IRS Model of Linguistic Comprehension
\end_layout

\begin_layout Section*
6.
 Applications to Question-Answering Tasks
\end_layout

\begin_layout Standard
To benchmark the IRS model, I propose to test its performance on existing
 question-answering datasets that include a wide range of question types.
 This benchmarking procedure has three stages.
 In the first stage, the model is tested on a recently released bAbl dataset
 of question-answer pairs designed to test for a range of abilities that
 are arguably prerequisites for linguistic understanding (Weston et al.,
 2015).
 For example, some questions require an agent or model to extract relevant
 information from a set of supporting facts (e.g.
 a sentence or short paragraph).
 Other questions require the use of numerical reasoning and the ability
 to respond to negations appropriately.
 That said, many of the bAbl questions are comparatively simple, requiring
 only sensitivity to structural information concerning different parts of
 speech in a supporting sentence.
 As such, this dataset is used as preliminary test for IRS model's linguistic
 comprehension abilities.
 Example question-answer pairs are shown in Table 1.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="7" columns="4">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Dataset
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Background Information
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Question 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Answer
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
bAbl
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Quotes eld
\end_inset

The office is north of the kitchen.
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
What is north of the kitchen?
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
The office.
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
bAbl
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
“Yann is hungry.”
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Where will Yann go?
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
The kitchen.
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Winograd
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Quotes eld
\end_inset

The trophy would not fit in the suitcase because it was too big/[small].
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
What was too big?
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
The trophy.
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Winograd
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Quotes eld
\end_inset

Joan made sure to thank Susan for all the help she had received/[given].
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Who received the help?
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Joan.
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Novel
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
“John is eating soup at the restaurant”
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
What did John use to eat the soup?
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
A spoon.
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Novel
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
“John is eating soup at the restaurant”
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Did John prepare the soup?
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
No.
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Example questions and answers for each proposed benchmarking dataset.
 The bAbl dataset is described in Weston et al.
 (2015).
 The Winograd Schema dataset is described in ?? (2013).
 The final, novel dataset will developed using human annotators on a platform
 such as Mechanical Turk.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Provided that this preliminary test is successful, the next stage of the
 benchmarking process involves testing the model on a class of questions
 that require the use of more complex forms of background knowledge.
 Specifically, a model or agent must use such knowledge to correctly identify
 the referent of an ambiguous pronoun in a sentence that matches a set of
 conditions inspired by seminal work on natural language processing by Terry
 Winograd (see Levesque, 2011).
 These conditions are as follows: the sentence must contain two noun phrases
 describing unique entities, a single pronoun that refers to one of these
 two entities, and a single word that can swapped with an alternative word
 to change the referent of the pronoun.
 Achieving good performance on this dataset is widely viewed to 
\end_layout

\begin_layout Standard
The final stage of benchmarking involves using questions that extend beyond
 pronoun resolution while still requiring considerable background knowledge.
 For example, the last two rows in Table 1 describe questions about a scenario
 in which person eats soup at a restaurant.
 Correctly answering these questions requires knowing that soup is liquid
 (and therefore must be eaten with a spoon) and that restaurants have staff
 that prepare meals (therefore implying that a patron would not prepare
 the soup).
 It will likely be necessary to be generate a new dataset for this portion
 of the project, given that there is fairly little prior work focused on
 tackling the problem of selecting background knowledge that is relevant
 to answering a particular question.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Most well known question-answering datasets are focused on factual questions
 such as 
\begin_inset Quotes eld
\end_inset

Who is Barack Obama's mother?
\begin_inset Quotes erd
\end_inset

.
 Performing well on these datasets arguably has more to do with translating
 a question into an appropriate database query than with exploiting background
 knowledge in a contextually appropriate way.
 
\end_layout

\end_inset

 To compensate for this lack of prior work and to assess the model’s performance
 on tasks of interest, I will generate a small dataset of question and answer
 pairs that require the sophisticated use of background knowledge.
 To keep the project tractable, this dataset will only include domain-specific
 questions (e.g.
 perhaps concerning meal-time activities and food) with a restricted degree
 of syntactic complexity.
 The point of these benchmarks is to make progress towards understanding
 how cognitive systems are able to flexibly and automatically determine
 which pieces of knowledge need to be applied in order to correctly answer
 a given question.
 
\end_layout

\begin_layout Standard
My overall evaluation of the IRS model will include performance comparisons
 to a number of baseline and alternative methods for performing question
 answering.
 Possible competing models include Weston et al's (2015) memory networks,
 a standard recurrent network for sequence prediction (e.g.
 Elman, 1990), and a semantic parser of the sort described in Liang et al.
 (2013).
 Having hopefully shown that the IRS model's performance on these datasets
 offers a strong empirical case in favor of satisfying the criteria outlined
 in Chapter 1, I go on to conclude with some final philosophical remarks.
 
\end_layout

\begin_layout Section*
7.
 Philosophical and Technical Reflections
\end_layout

\begin_layout Standard
The concluding chapter of the thesis is concerned with revisiting a number
 of philosophical and technical debates concerning semantics in light of
 the results described in the previous chapters.
 On the philosophical front, there are two key features of my framework
 that are open to criticism.
 First, my account is committed to a version of inferential role semantics,
 and philosophers have raised a number of well-known
\end_layout

\begin_layout Section*
Conclusion
\end_layout

\end_body
\end_document
