#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Semantic Composition and Models of Linguistic Comprehension
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
This thesis is concerned with the problem of ascribing meanings to expressions
 in a natural language.
 Traditionally, philosophers have attempted to solve this problem by proposing
 views on which a meaning is a particular kind of entity (e.g.
 a proposition, a referent, a mental state etc.).
 The job of a semantic theory, as such, is to pair linguistic expressions
 with the entities that constitute their meanings.
 I argue for an alternative view on which meaning attributions are simply
 concise ways of expressing predictive generalizations about cognitive and
 behavioral phenomena.
 To give an account of the meaning of a linguistic expression, I claim,
 is simply to give a set of probabilistic expectations concerning the cognitive
 and behavioral effects of its use.
 This empirically-oriented approach to semantics gives rise to a number
 of theoretical desiderata that I outline in Chapter 1.
 In Chapter 2, I argue that these desiderata constrain the space of potential
 semantic theories to include only those that make representational contents
 dependent on the functional roles of representational vehicles.
 Chapter 3 offers a reconsideration of the principle of compositionality,
 a basic theoretical assumption in both cognitive science and linguistics
 that is widely assumed to be inconsistent with any account of semantics
 that appeals to functional roles.
 
\end_layout

\begin_layout Standard
Part 2 of the thesis is concerned with providing a formal model of linguistic
 comprehension that satisfies the desiderata discussed in Part 1.
 First, I examine existing alternatives.
 I conclude that these alternatives are generally either compositional and
 inadequately functionalist, or functionalist and inadequately compositional.
 I go on to develop an account on which linguistic comprehension is modelled
 as a sequence of cognitive processes initiated by the perception of a sequence
 of linguistic expressions.
 The perception of a word, for instance, initiates a procedure that puts
 a cognitive system into a state that licenses the syntactic and semantic
 functions of the word.
 As further word perceptions initiate further procedures of this sort, the
 system's resulting state can be characterized as a 
\begin_inset Quotes eld
\end_inset

mental model
\begin_inset Quotes erd
\end_inset

 that satisifies the functional specification (i.e.
 the predictions) associated with a particular semantic attribution for
 a phrase or sentence.
 Chapter 5 describes a method for generating of this mental model from linguisti
c input using a hierarchical constraint satisfaction procedure.
 Each word in the input stream favors or penalizes the inclusion or particular
 structural components in the resulting model.
 At lower levels in the hierarchy, the structural components in questions
 correspond to collections of the kinds of semantic and syntactic features
 typically found in contemporary linguistic theory; at higher levels in
 the hierarchy, the structural components correspond to collections of compresse
d approximations of lower-level components.
 The resulting model can be described as a 
\begin_inset Quotes eld
\end_inset

shallow
\begin_inset Quotes erd
\end_inset

 semantic parse that captures abstract information concerning 
\begin_inset Quotes eld
\end_inset

who did what to whom
\begin_inset Quotes erd
\end_inset

 (to use a common phrase).
 
\end_layout

\begin_layout Standard
Mathematically, each structural component can be characterized as a high-dimensi
onal vector, and the constraints defined on these components can be characterize
d as scalar-valued functions on the underling vector space.
 (Psychologically, it is possible to think of these structural components
 as representations retrieved from long-term memory in service of the generation
 of a contextually appropriate representation in short-term memory.) Finally,
 I illustrate how the constraints in the model can learned via standard
 optimization methods.
 The result of this learning procedure is a parameterization of the model
 that allows a linguistic input to be translated into a structure that possess
 the functional dispositions that constitute its meaning.
 These functional dispositions are assessed through simple question-answering
 tasks that require the model to appropriately respond to queries concerning
 a single sentence or paragraph.
 
\end_layout

\begin_layout Standard
Part 3 of the thesis is concerned with practical applications and theoretical
 reflections.
 First, I apply the model to a standard question-answering data set.
 I aim to achieve near state-of-the-performance on this dataset in comparison
 to the other methods discussed in Part 1.
 In the remainder of the thesis, I compare and contrast the formal and philosoph
ical details of my work with the alternatives available in the literature.
 Specifically, I endeaver to show that while the formal details of my approach
 can be given equivalent probabilistic and control-theoretic interpretations.
 On a more philosophical front, I illustrate how my work is both consistent
 with standard descriptions of inferentialist semantic theories and resistant
 to the criticisms typically lodged against these theories.
 I conclude, finally, with a brief discussion of future directions for this
 research.
 
\end_layout

\begin_layout Section*
1.
 A Cognitive Approach to Semantics
\end_layout

\begin_layout Quote
In order to say what a meaning 
\shape italic
is
\shape default
, we may first ask what a meaning 
\shape italic
does
\shape default
, and then find something that does that.
 - David Lewis, 1970
\end_layout

\begin_layout Standard
There seems to be no shortage of things for meanings to do.
 On the one hand, they are given the rather theoretical job of connecting
 words to objects in the world and specifying the truth conditions of sentences.
 On the other hand, they are given the rather practical job of explaining
 linguistic and cognitive behavior.
 Philosophers have put a lot of effort into finding entities that are able
 to satisfy these job descriptions, but to relatively little avail.
 In practice, more or less distinct theories have been developed to achieve
 more or less distinct goals.
 For instance, theories in formal semantics are designed to account for
 truth and reference, but not for linguistic behavior.
 Theories in pragmatics and psychology, by comparison, are designed to do
 roughly the opposite.
 As such, there appear to be two largely independent notions of meaning
 at play in the philosophical literature: the first notion - call it 
\begin_inset Quotes eld
\end_inset

metaphysical semantics
\begin_inset Quotes erd
\end_inset

 - is concerned with relations that obtain between representations and reality,
 while the second notion - call it 
\begin_inset Quotes eld
\end_inset

cognitive semantics
\begin_inset Quotes erd
\end_inset

 - is concerned with explaining how people comprehend and produce linguistic
 expressions.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
The tension between these two notions of meaning is well known.
 The related problems of integrating (formal) semantics with pragmatics
 and accounting for the apparent context-dependency of meaning can both
 be characterized as stemming from the differing goals of metaphysical and
 cognitive approaches to semantics.
 Moreover, researchers often explicitly argue that either metaphysical semantics
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
My goal in this chapter is to motivate a cognitive approach to semantics
 and outline a corresponding set of theoretical desiderata.
 There are two main reasons for favoring the cognitive approach.
 First, it accords well with a naturalistic conception of philosophy.
 To a naturalist, meanings can be thought of as entities posited by a scientific
 theory that aims to explain how and why we use linguistic expressions in
 the way that we do.
 Attributing a meaning to word is not unlike attributing a center of mass
 to an object or charge to a particle (cf.
 Dennett, 1987); in all such cases, the attribution is warranted because
 lends explanatory and predictive insight into certain phenomena of interest
 (i.e.
 phenomena involving words, objects, and particles respectively).
 Second, very little is lost by giving up on the considerations that have
 motivated the metaphysical approach.
 Placing reference and truth at the core of semantic theory has yielded
 well-known theoretical considerations on which the meaning of an expression
 or mental state can depend on facts entirely unknown to language users
 (e.g.
 Twin earth cases, Frege cases, etc).
 I argue that such considerations give rise to unsatisfiable and counterproducti
ve theoretical demands.
 Put simply, allowing distinctions in meaning to correspond to obscure distincti
ons in reality burdens semantic theory with a dependency on facts about
 the world that have no empirical consequences concerning language use.
 Morevoer, communal ignorance of such facts problematically translates into
 communal ignorance about the meanings of ordinary words and sentences.
\end_layout

\begin_layout Standard
As a result of these considerations favoring the cognitive approach, I propose
 a set of four criteria that can be used to evaluate competing semantic
 theories:
\end_layout

\begin_layout Enumerate
The predictive adequacy criterion.
 (Behavior)
\end_layout

\begin_layout Enumerate
The implementation criterion.
 (Cognition)
\end_layout

\begin_layout Enumerate
The compositionality criterion.
 (Generalization)
\end_layout

\begin_layout Enumerate
The scalabilty criterion.
 (Scope)
\end_layout

\begin_layout Standard
While these criteria are somewhat distinct from those commonly found in
 the literature, they should not be entirely surprising.
 The first criterion is meant to capture the idea that the attribution of
 a particular meaning to a linguistic expression is warranted only when
 doing so has scientific value.
 Such value, I contend, arises when the meaning attributed to a particular
 expression gives rise to successful predictions concerning (a) the cognitive
 states of individuals who produce or comprehend the expresssion, and (b)
 the verbal behavior of such individuals.
 To illustrate with an example, consider the following sentence: “The boy
 waited for the pitch and then hit the ball over the fence”.
 A competent speaker of English is able to rapidly infer from this sentence
 that the boy is likely playing baseball, that he has used a bat to hit
 the ball, and that the phrase “over the fence” refers to where the ball
 went after he hit it, rather than to the location of the ball when he hit
 it.
 The predictive adequacy criterion favors attributing a meaning to this
 sentence that entails that anyone who correctly understands it will provide
 appropriate answers to questions such as 
\begin_inset Quotes eld
\end_inset

Where did the ball go?
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

What did the boy likely use to hit the ball?
\begin_inset Quotes erd
\end_inset

 Finally, it is worth emphasizing that these predictions are most naturally
 understood in probabilistic rather than deductive terms: a meaning attribution
 does not 
\shape italic
entail
\shape default
 the truth of certain conterfactual conditionals concerning linguistic or
 cognitive phenomena; rather it 
\shape italic
favors
\shape default
 such conditionals in a violable manner.
\end_layout

\begin_layout Standard
The second criterion is meant to capture the idea that predictions licensed
 by a particular meaning attribution place certain requirements on things
 to which the predictions are applied (e.g.
 cognitive systems).
 For example, if a meaning attribution specifies that a certain phrase is
 likely to evoke a certain behavior in a listener, then the word and the
 behavior must be related such that this specification is met.
 However, there might be compelling evidence to suggest that a cognitive
 system is simply unable to mediate between words and behaviors in the specified
 way.
 Such evidence ought to prompt a revision to the proposed meaning attribution.
 Similarly, if there is more compelling evidence to suggest that the behavioral
 expectations in question actually obtain, then a revision to one's account
 of cognition ought to be considered.
 Overall, the implementation criterion simply favors the development of
 theories of linguistic behavior and linguistic cognition that are consistent
 with one another.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
As a brief technical aside, it can be useful to think of the implementation
 criterion through analogy to the Neural Engineering Framework (NEF).
 Models built using the NEF typically assume that neural populations are
 responsible for computing functions on a latent vector space through their
 neural activities.
 Hypothesizing that a particular neural ensemble computes a particular function
 is highly analogous to making a meaning attribution.
 To explain, the hypothesis generates a large number of predictions concerning
 the values that can be decoded from the ensemble under various conditions.
 Likewise, the hypothesis also places constraints on the properties of the
 neurons in the ensemble.
 For instance, their tuning curves need to tile the latent vector space
 in a manner that permits computing the hypothesized function.
 (These tuning curves are determined both by the intrinsic properties of
 the neurons and their incoming connection weights).
 Overall, just as evidence concerning function and implementation mutually
 constrain one another in the development of neurocomputational theories,
 so too do they constraint one another in the development of semantic theories.
 This all should come as no surprise given the representational considerations
 that underlie the NEF.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The third criterion is widely accepted in discussions of semantics, and
 is meant to account for the fact that people are able to generalize their
 understanding of the meanings of simple linguistic expressions (and syntax)
 to an understanding of arbitrarily large numbers of more complicated expression
s.
 In typical discussions of semantics, this productive capacity is characterized
 in terms of the recursive application of a finite (and therefore learnable)
 set of semantic and syntactic rules.
 While I do not wish to adhere strictly to this traditional picture, I do
 wish to maintain that any adequate semantic theory has to provide some
 explanation of how the meanings of complex expressions (e.g.
 sentences) are related to the meanings of their simpler parts (e.g.
 words and phrases).
 
\end_layout

\begin_layout Standard
The scalability criterion gives preference to semantic theories that account
 for the meanings of large numbers of linguistic expressions.
 Of course, it is not particular controversial to say that scientific theories
 with a broad range of empirical coverage are preferable to scientific theories
 with a smaller range of coverage.
 But when considered in tandem with the other criteria listed above, scalability
 concerns become more prominent.
 For example, some approaches to semantics are able to achieve compositionality
 along with a certain degree of predictive adequacy, but only in highly
 restricted domains.
 A salient case would be work in formal semantics that handles a limited
 range phenomena focused on things like generalized quantification and coordinat
ion.
 At best, this approach covers a small fraction of the possible linguistic
 expression types, and efforts to increase the scope of the approach have
 been largely unsuccessful.
\end_layout

\begin_layout Standard
In the remainder of the chapter, I consider two objections the selection
 of these criteria.
 The first objection maintains that they favor a kind of instrumentalism
 that is typical of theories that prioritize predictive adequacy over other
 concerns (see e.g.
 Dennett, 1987).
 My response is that if the distinctions between two competing meaning attributi
ons have no empirical basis, then it is unclear that there reasons to prefer
 one over the other.
 In other words, I am taking Lewis to heart and claiming that if two competing
 meaning attributions 
\shape italic
do
\shape default
 the same work, then for all intents and purposes, they 
\shape italic
are
\shape default
 the same.
 If this amounts to an instrumentalist view, then so be it.
\end_layout

\begin_layout Standard
The second objection claims that the cognitive approach entails a form of
 meaning relativism.
 Again, I am willing to concede the point to an extent.
 Words do mean different things to different people, because usage of a
 given word can sustain different predictive generalizations in different
 individuals and different social groups.
 However, the fact language use is a social practice militates against widesprea
d relativism.
 People communicate to achieve practical goals, and achieving these such
 goals requires that people be able to make good predictions in relation
 to the usage of particular linguistic expressions; hence, people converge
 on shared patterns of usage that foster predictive success.
 As such, a tempered form of relativism is to be expected rather than avoided.
 
\end_layout

\begin_layout Standard
In summary, the main purpose of this chapter is to set the stage for the
 development of an approach to semantics that is both philosophically robust
 and integrated with the broader research goals of cognitive science.
 In the next chapter, I evaluate a number of existing methods in the literature
 to see how well they fare against my criteria.
\end_layout

\begin_layout Section*
2.
 Formal Methods and the State of the Art
\end_layout

\begin_layout Quote
If you take care of the syntax of a representational system, its semantics
 will take care of itself.
 - Haugeland 
\end_layout

\begin_layout Standard
In this chapter, I survey a number of existing approaches to modeling linguistic
 phenomena and evaluate how well they fare in light of the criteria introduced
 in Part 1.
 I highlight the strengths and weaknesses of each approach and provide a
 brief summary of open problems that need to be solved in order to achieve
 progress.
 
\end_layout

\begin_layout Standard
The analysis proceeds in three stages.
 First, I evaluate contemporary work in formal semantics.
 Inspired by developments in mathematical logic, this work typically assumes
 that natural languages can be given model-theoretic interpretations of
 the kind given to formal languages.
 A model, to explain, maps each term in a language to an object in a mathematica
l structure.
 For example, a model of a first-order language would consist of a domain
 of individuals (a “universe of discourse”), a domain of truth values, and
 an interpretation function that maps each term in the language to either
 an individual in the domain, an n-ary function, or an n-ary relation (Goldfarb,
 2003; Carpenter, 1997).
 Once an interpretation of the language is fixed in this way, truth values
 are generated for all sentences formed using the standard logical connectives
 and quantifiers.
 Moreover, each sentence in the language can be understood in terms of the
 set of intepretations that make it true: for a particular sentence S, each
 interpretation of S that makes S true is considered to be a model of S
 (Hodges, 2013).
 Thus, the set of models of S specifies the truth-conditions for S, and
 for any particular model, S will be either true or false.
 
\end_layout

\begin_layout Standard
To their credit, formal semanticists have produced influential analyses
 of a variety of linguistic phenomena involving things like quantification,
 tense, modality, and anaphora.
 But it should be obvious that this work is unsatisfactory from the perspective
 of the criteria introduced in Chapter 1.
 Most importantly, denotational theories typically have little predictive
 adequacy.
 They are simply not designed to account for the social and psychological
 aspects of language use.
 To illustrate with an example, a standard denotation to give the word 
\begin_inset Quotes eld
\end_inset

table
\begin_inset Quotes erd
\end_inset

 is the set of things that are tables.
 Needless to say, this set of objects does not give rise to the sort of
 expectations that would satisfy the predictive adequacy criterion unless
 numerous further assumptions are made concerning how these objects are
 represented in the minds of language users.
 Some see this abstraction away from empirical considerations as a virtue
 (e.g.
 Lewis, 1970), while others see it as a flaw in need of a remedy (e.g.
 Partee, 1980; Jackendoff, 2004).
 
\end_layout

\begin_layout Standard
Nonetheless, it is important to note that the methods of formal semantics
 are not intrinsically incapable of producing good explanations of cognitive
 and linguistic phenomena.
 These methods are committed to the idea that meanings are denotations,
 but the denotations in question need not be mathematical objects: they
 could be states in a cognitive model, computer programs, linguistic expressions
, or any number of other things (Liang & Potts, 2015).
 Recent work has exploited this looser notion of denotation in tandem with
 state-of-the-art machine learning techniques to develop systems that parse
 questions expressed in natural language onto 
\begin_inset Quotes eld
\end_inset

logical forms
\begin_inset Quotes erd
\end_inset

 that denote appropriate answers extracted from a knowledge base (Liang,
 Jordan, & Klein, 2013).
 Most impressively, the mapping from linguistic expressions to logical forms
 is learned strictly from examples of correct pairings between expressions
 and their denotations.
 Such learning procedures might be naturally extended to develop cognitive
 models that correctly mediate between certain inputs (linguistic expressions)
 and their denotations (other linguistic expressions) in an empirically
 adequate way.
 Overall, I conclude from this evaluation that formal semantics has certain
 strengths that can inform the development of any adequate theory.
 It does very well with respect to compositionality, and recent efforts
 have shown great promise with respect to scalability.
 The weaknesses, on the other hand, are still somewhat prohibitive.
 Predictive adequacy and implementational plausibility exist only in the
 realm of speculation.
 
\end_layout

\begin_layout Standard
In the next stage of my analysis, I examine what are referred to as 
\begin_inset Quotes eld
\end_inset

distributional
\begin_inset Quotes erd
\end_inset

 models of meaning.
 These models take as their point of departure the idea that the meaning
 of a word is captured by the contexts in which it occurs (Landauer & Dumais,
 1997; Grefenstetet, 2013).
 In practice, the models are generated by collecting frequency counts for
 word occurrences across a wide range of linguistic contexts (e.g., a collection
 of documents or paragraphs), and then using these frequency counts to form
 vectors that reflect the frequency profile of each word (Clark, 2014; Turney
 and Pantel, 2010).
 The vectors that result from this process possess geometric relationships
 that correspond to semantic relationships of various kinds.
 For instance, the vectors for words associated with a particular abstract
 category (e.g.
 sports, food, animals, etc.) will tend to cluster together.
 This semantically informed clustering allows one retrieve words semantically
 related to a target word through a ranking of similarities between the
 underlying vectors.
 
\end_layout

\begin_layout Standard
Distributional models of meaning have proven quite effective at accounting
 for a wide range of empirical data concerning word usage.
 For example, they have achieved success matching human data from studies
 of category typicality (e.g., Jones & Mewhort, 2007) and synonym identification
 (e.g., Landauer & Dumais, 1997).
 Good matches to human data on tasks involving stem-completion (Jones &
 Mewhort, 2007) and judgments of phrasal similarity (Mitchell & Lapata)
 have also been achieved.
 Given these matches, it is clear that the distributional approach does
 reasonably well at satisfying the predictive adequacy criterion.
 And given that distibutional models involving tens of thousands of words
 generated from millions of documents have been developed, it is also clear
 that the approach can scale effectively.
 However, a major shortcoming of these model is that sentences and paragraphs
 are often treated as a 
\begin_inset Quotes eld
\end_inset

bag of words
\begin_inset Quotes erd
\end_inset

 that contains no synactic structure at all.
 More generally, it has proven very difficult to generate compositional
 representations of phrases and sentences using the distributional approach.
 
\end_layout

\begin_layout Standard
However, it also true that a great deal of effort has been directed towards
 developing techniques for combining distributed representations into complex
 structures.
 Early examples of these techniques include Smolenksy’s (1990) tensor products,
 Kanerva’s (1994) binary spatter codes, and Plate’s (2003) holographic reduced
 representations.
 In each case, a mathematical operation is used to combine two vectors in
 way that allows each constituent vector to later be recovered in either
 exact or approximate form.
 One problem with these techniques is that they arguably group representations
 together rather then compose them (i.e.
 binding imposes syntactic structure in the absence of a correspondingly
 appropriate semantic structure).
 As such, more recent efforts to achieve compositionality have made use
 of either (a) recursively structured neural networks that function to merge
 distributed representations, or (b) tensor-based representations that can
 composed via operations analogous to function application of the sort character
istic of work in traditional formal semantics.
 I briefly evaluate these approaches and argue that, while they are effective
 at solving specific kinds of problems, they do not provide general purpose
 representations of composite linguistic structures.
 
\end_layout

\begin_layout Standard
Finally, I evaluate work from the 
\begin_inset Quotes eld
\end_inset

connectionist
\begin_inset Quotes erd
\end_inset

 literature that makes use of artificial neural networks.
 I focus primarily on two bodies of work.
 
\end_layout

\begin_layout Standard
For example, I discuss Rogers and McClelland's (2004, 2008) influential
 model of semantic cognition and concept learning.
 This model has a considerable amount of predictive power, since it is able
 to account for a wide range of data concerning categorization, taxonomy
 induction, and lingustic inference.
 The model is also loosely consistent with implementational constraints
 provided by cognitive systems given that it makes use of neuron-like processing
 units.
 As such, the model does a reasonably good job of satsifying criteria (1)
 and (2).
 More importantly, its representational states are uncontroversially defined
 in terms of their relationships to one another.
 For instance, the model's hidden layer representations are defined via
 a learning algorithm to properly mediate between abstract representations
 of perceptual inputs and abstract representations of the properties that
 are characteristic of these inputs.
 But virtues aside, the model clearly fails to address the compositionality
 criterion: it provides no description of a mechanism for binding or conjoining
 representational states.
 I provide similar evaluations of connectionist models of parsing (Hale
 & Smolensky, 2006; Elman, 1990), pronunciation (Plaut et al., 1996), and
 more computationally-oriented tasks such as sentiment-analysis and knowledge
 base completion (Socher et al.
 2012).
 
\end_layout

\begin_layout Section*
3.
 Reconsidering Compositionality
\end_layout

\begin_layout Quote
It is enough if the sentence as whole has meaning; thereby also its parts
 obtain their meanings - Frege, qtd.
 in Szabo, 2012
\end_layout

\begin_layout Standard
The principle of compositionality states that the meaning of a complex expressio
n is determined by the meanings of its constituents and the way they are
 combined (Szabo, 2012; Fodor & Pylyshyn, 1988).
 In the study of natural language semantics, this principle is often taken
 to entail that expressions such as phrases and sentences are interpreted
 in the following two-step manner (Recanati, 2012): first, a set of a lexical
 rules are used to assign meanings to individual words; then, a set of compositi
on rules are used to combine these simple meanings into the meaning of a
 phrase or sentence.
 
\end_layout

\begin_layout Standard
Perhaps unsurprisingly, the principle is widely seen to pose a problem for
 functional role semantics.
 Philosophers have provided numerous examples of complex linguistic expressions
 that appear to have functional roles unrelated to the functional roles
 of their constituent parts (e.g Fodor and Lepore, 1991; Fodor, 1998).
 Moreover, there is a long history of successful efforts to formalize denotation
al semantics that has simply not been matched by propents of functional
 role semantics (c.f.
 Pagin, 1997).
 This lack of theoretical progress has led to considerable skepticism about
 the viability of functionalist theories.
 
\end_layout

\begin_layout Standard
My goal in this chapter is to provide a counterweight to such skepticism.
 Specifically, I propose a view on which simple linguistic representations
 individually impose numerous constraints on a search through a space of
 entities with particular functional dispositions.
 Taken collectively, these constraints pick out an entity with a functional
 disposition that accords with the semantics of a complex linguistic expression.
 The process of optimizing over these soft constraints is hypothesized to
 occur at different levels of abstraction, allowing for compositional constructi
ons of varying degrees of complexity.
 To give an example, a determiner and a noun might together impose constraints
 on the selection of an entity with the functional disposition appropriate
 to a noun phrase.
 In turn, the resulting noun-phrase can in turn be characterized as imposing
 constraints on the selection of an entity with the functional disposition
 of a sentence.
 This sentence-level selection process can then be resolved by the introduction
 of further constraints corresponding to another sentential constituent
 (e.g.
 a verb phrase).
 Developing a semantic theory, as such, involves specifying the candidate
 entities at each level of syntactic abstraction and then defining the constrain
ts that select amongst these various candidates in various circumstances.
 A mathematically precise version of such a theory is provided in Part 2
 below.
 
\end_layout

\begin_layout Standard
In the remainder of the chapter, I give three reasons for favoring this
 constraint-based description of semantic composition.
 First, it is theoretically consistent with the criteria outlined in Chapter
 1.
 If meaning attributions are simply predictive generizations associated
 with the use of particular words and sentences, then complex meanings are
 not 
\begin_inset Quotes eld
\end_inset

built up
\begin_inset Quotes erd
\end_inset

 from simpler ones through a process analogous to the construction of wall
 from a collection of bricks (citation).
 The probabilistic expectations corresponding to a simple linguistic expression
 instead provide defeasible evidence in favor of associating certain probabilist
ic expectations with a more complex linguistic expression.
 By comparison, standard approaches to compositionality relate the meanings
 of simple and complex expressions through simple rules, but it is implausible
 to suppose that such rules can suffice for a semantic theory that satisfies
 the predictive adequacy criterion.
 To explain with an example, the semantic composition of certain adjectives
 and nouns is standardly modeled as the intersection of the sets denoted
 by each noun and adjective.
 It is highly unlikely that a comparably simple operation can be used to
 
\begin_inset Quotes eld
\end_inset

compose
\begin_inset Quotes erd
\end_inset

 the functional dispositions associated with words in these types.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Suppose, for instance, that these functional dispositions are characterized
 as sets of predictions.
 Why think that the predictions associated with a complex expression would
 be all and only the predictions shared by each of its simpler constituents?
 Suppose alternatively that the functional dispositions in question can
 be characterized as probability distributions over verbal behaviors or
 cognitive states.
 Why think that a simple rule would suffice to combine such distributions
 in the appropriate way? 
\end_layout

\end_inset

 An optimization-based approach, however, has the necessary flexibility
 to define the appropriate formal relations between these functional disposition
s.
 
\end_layout

\begin_layout Standard
Second, the constraint-based approach provides a plausible solution to the
 problem of accounting for context-sensitivity within a compositional semantics.
 An important challenge to the idea that natural languages are compositional
 arises from the fact that many expression types seem to take on different
 meanings in different contexts 
\begin_inset CommandInset citation
LatexCommand citep
key "Recanati:2005,Unnsteinsson:2014"

\end_inset

.
 Consider, for example, the following sentence: 
\begin_inset Quotes eld
\end_inset

Mike finished John's book
\begin_inset Quotes erd
\end_inset

 (Recanati, 2004, p.
 62).
 It is highly plausible that the meaning of this sentence can vary depending
 on whether Mike finished reading the book or finished binding it 
\begin_inset CommandInset citation
LatexCommand citep
key "Recanati:2005"

\end_inset

.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
The genitive construction 
\begin_inset Quotes eld
\end_inset

John's book
\begin_inset Quotes erd
\end_inset

 can also exhibit variation depending on whether, for instance, John wrote
 the book or John owns the book.
\end_layout

\end_inset

 But if the verb 
\begin_inset Quotes eld
\end_inset

finished
\begin_inset Quotes erd
\end_inset

 is assigned a fixed meaning in accordance with a semantic rule, then it
 is not clear how to account for this variation, since it cannot be attributed
 to a change in syntactic structure or a modification of the lexical constituent
s in the sentence.
 However, if it is assumed that individual words act to impose numerous
 constraints on the process of linguistic interpretation, then it follows
 that the behavior of a given word is conditioned by the other words in
 the sentence.
 As such, a word like 
\begin_inset Quotes eld
\end_inset

finished
\begin_inset Quotes erd
\end_inset

 can exhibit different kinds of compositional behavior when it occurs in
 different linguistic constructions.
 Moreover, extra-linguistic features of context can also be hypothesized
 to impose constraints on the selection process.
 For example, constraints imposed by the topic of prior discourse could
 resolve the ambiguity present in the above utterance involving John's book
 by biasing a person who heard the utterance towards responding to particular
 questions with particular answers (e.g.
 
\begin_inset Quotes eld
\end_inset

Was the book an enjoyable read?
\begin_inset Quotes erd
\end_inset

).
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
One might argue that the impact of contextual features on semantic composition
 could be accounted for by a system of rules that are sensitive to their
 context of application.
 This is true in principle, but the rules in question would have to be explicity
 defined for each possible pairing of a context and a linguistic expression.
 In practice, enumerating such a large number of rules is intractable.
 The constraint-based approach gets around this problem by having the effect
 of each hypothesized rule fall of out an optimization process that is defined
 only in relation to a fixed inventory of constraints associated with contexts
 and linguistic expressions individually (i.e.
 not considered as pairs - the difference is between 2n sets of constraints
 and 2^n rules, i.e.
 linear vs.
 exponential in the number of contexts and expressions).
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A third reason to favor a notion of compositionality that is consistent
 with the predictive adequacy criterion is that doing so usefully allows
 for 
\begin_inset Quotes eld
\end_inset

top down
\begin_inset Quotes erd
\end_inset

 effects on interpretation.
 In accordance with Frege's 
\begin_inset Quotes eld
\end_inset

context principle
\begin_inset Quotes erd
\end_inset

, one could envision a situation in which the functional disposition associated
 with a particular word during the initial interpretation of sentence differs
 from the functional disposition associated with the same word upon the
 completion of interpretation.
 Traditional accounts of semantic composition are inconsistent with this
 possibility.
 
\end_layout

\begin_layout Standard
Finally, I conclude the chapter by noting that my proposal is actually somewhat
 conservative: it does not require any radical reinterpretion the principle
 of compositionality.
 To explain, the principle merely states that there is some systematic relations
hip between the meanings of simple and complex linguistic expressions.
 To borrow an example from Szabo, meanings could be tables and chairs at
 no risk of violating the principle so long as the tables and chairs assigned
 to simple linguistic expressions are appropriately related to the tables
 and chairs assigned to more complex expressions.
 However, there seems to be a widespread assumption to contrary - namely
 that a semantic theory can only be compositional if it specifies of set
 of rules or functions that combine simple meanings into more complex ones.
 I hope to have cast doubt on this assumption by showing how it leads to
 a view of linguistic cognition that is both empirically problematic and
 theoretical restrictive.
 
\end_layout

\begin_layout Section*
4.
 A General Framework for Modeling Linguistic Phenomena 
\end_layout

\begin_layout Standard
In this chapter, I take up the task of appropriately formulating the problem
 that models of linguistic phenomena are designed to solve.
 
\end_layout

\begin_layout Section*
5.
 The IRS Model of Linguistic Comprehension
\end_layout

\begin_layout Standard
- add parameterization procedure
\end_layout

\begin_layout Section*
6.
 Applications to Question-Answering Tasks
\end_layout

\begin_layout Section*
7.
 Philosophical Reflections
\end_layout

\begin_layout Standard
The goal of this chapter is to revisit some of the philosophical debates
 surrounding semantics in light of the results presented in the previous
 two chapters.
 
\end_layout

\begin_layout Standard
Next, I evaluate probabilistic models that describe linguistic representations
 as intuitive theories.
 Building on psychological research that emphasizes the importance of background
 knowledge on conceptual processing, these models describe processes that
 generate the observations predicted by a particular folk theory.
 For example, ...tug of war example...In some respects, these models are quite
 successful.
 They account for much of the existing data on knowledge effects, and they
 have produced a number of successful predictions.
 Moreover, the models are compositional in the sense that the generative
 processes giving rise to particular observations can be combined in a structure
d manner to yield a more complicated generative process.
 Nonetheless, implementation and scalability issues tend pose problems for
 these models.
 With respect to implementation, the models simply have nothing to say;
 they are simply formal specifications of a set functions being computed
 by a cognitive system.
 With respect to scalability, there is no indication that models describing
 the behavior of proper sentences can be generated using these probabilistic
 methods.
 Existing work abstracts away from the complexity of linguistic expressions
 to focus on small numbers of independent words.
 I conclude from these observations that 
\end_layout

\end_body
\end_document
